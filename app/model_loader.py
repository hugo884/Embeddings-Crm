from sentence_transformers import SentenceTransformer
import os
import torch
import logging
import psutil
import numpy as np
from typing import List

logger = logging.getLogger(__name__)

class EmbeddingModel:
    def __init__(self, torch_config=None):
        self.model_name = os.getenv("EMBEDDING_MODEL", "paraphrase-multilingual-mpnet-base-v2")
        self.loaded = False  # Bandera para indicar si el modelo est√° cargado
        
        # Determinar dispositivo autom√°ticamente
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        
        # Manejo de precisi√≥n m√°s robusto
        self.precision = self._determine_precision(torch_config)
        
        logger.info(f"‚è≥ Loading model '{self.model_name}' on {self.device} ({self.precision})...")
        
        try:
            # Par√°metros de carga optimizados
            load_params = {
                "device": self.device,
                "cache_folder": "./model_cache"
            }
            
            # Configuraci√≥n de precisi√≥n para GPU
            if self.precision == "fp16" and self.device == "cuda":
                load_params["torch_dtype"] = torch.float16
            
            # Cargar modelo principal
            self.model = SentenceTransformer(self.model_name, **load_params)
            self.model_dimensions = self.model.get_sentence_embedding_dimension()
            
            # Manejo de cuantizaci√≥n m√°s seguro
            self.quantized_model = None
            if self.precision == "quantized":
                self._initialize_quantized_model()
            
            self.loaded = True  # Marcar como cargado exitosamente
            logger.info(f"‚úÖ Model loaded | Dimensions: {self.model_dimensions}")
        except Exception as e:
            logger.critical(f"‚ùå Error loading model: {str(e)}", exc_info=True)
            raise RuntimeError(f"Model loading failed: {str(e)}")

    def _determine_precision(self, torch_config):
        """Determina la precisi√≥n √≥ptima basada en configuraci√≥n y hardware"""
        if self.device == "cuda":
            return "fp16"
        
        if torch_config and torch_config.get("quantize", False):
            return "quantized"
        
        return "fp32"

    def _initialize_quantized_model(self):
        """Inicializa modelo cuantizado solo si es necesario y posible"""
        try:
            logger.info("üîÑ Applying model quantization...")
            from optimum.onnxruntime import ORTModelForFeatureExtraction
            self.quantized_model = ORTModelForFeatureExtraction.from_pretrained(
                self.model_name, 
                export=True
            )
            logger.info("‚úÖ Quantization applied successfully")
        except ImportError:
            logger.warning("‚ö†Ô∏è ONNX Runtime not installed, using standard model")
            self.precision = "fp32"
        except Exception as e:
            logger.error(f"‚ö†Ô∏è Quantization failed: {str(e)}")
            self.precision = "fp32"

    def _calculate_optimal_batch_size(self, texts: List[str]) -> int:
        """Calcula el tama√±o de lote √≥ptimo basado en memoria disponible"""
        mem = psutil.virtual_memory()
        free_mem = mem.available
        
        # Estimaci√≥n de memoria requerida (4 bytes por float * dimensiones * longitud promedio)
        avg_text_len = sum(len(t) for t in texts) / max(1, len(texts))
        estimated_mem_per_text = self.model_dimensions * 4 * (avg_text_len / 4)  # Asume 4 caracteres ‚âà 1 token
        
        # C√°lculo seguro considerando memoria libre
        max_batch_size = max(1, int(free_mem / (estimated_mem_per_text * 2.5)))  # Factor de seguridad 2.5x
        
        # Limitar por cantidad de textos y m√°ximo razonable
        return min(len(texts), max(8, min(128, max_batch_size)))

    def encode(self, texts: List[str], **kwargs) -> np.ndarray:
        """Codifica textos con manejo autom√°tico de recursos"""
        if not self.loaded:
            raise RuntimeError("Model not loaded")
        
        # Calcular batch_size √≥ptimo si no se especifica
        if "batch_size" not in kwargs:
            kwargs["batch_size"] = self._calculate_optimal_batch_size(texts)
            logger.debug(f"Using automatic batch size: {kwargs['batch_size']}")
        
        # Usar modelo cuantizado si est√° disponible
        if self.precision == "quantized" and self.quantized_model:
            return self.quantized_model.encode(texts, **kwargs)
        
        # Manejo de precisi√≥n para GPU
        if self.precision == "fp16" and self.device == "cuda":
            kwargs["convert_to_tensor"] = True
            embeddings = self.model.encode(texts, **kwargs)
            return embeddings.half().cpu().numpy()
        
        # Caso est√°ndar
        return self.model.encode(texts, **kwargs)
    
    def get_dimensions(self) -> int:
        """Devuelve las dimensiones del embedding"""
        return self.model_dimensions
    
    def is_loaded(self) -> bool:
        """Indica si el modelo est√° cargado y listo para usar"""
        return self.loaded