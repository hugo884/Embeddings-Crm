import time
import math
import numpy as np
import psutil
import logging
from concurrent.futures import ThreadPoolExecutor
from typing import List, Tuple
from ..utils.cache_utils import EmbeddingCache
from ..model_loader import EmbeddingModel

logger = logging.getLogger(__name__)

class EmbeddingService:
    def __init__(
        self,
        model: EmbeddingModel,
        cache: EmbeddingCache,
        executor: ThreadPoolExecutor
    ):
        self.model = model
        self.cache = cache
        self.executor = executor

    async def embed_texts(self, texts: List[str]) -> Tuple[List[List[float]], int, float, int]:
        """
        Devuelve:
            - embeddings: Listado de vectores (floats)
            - cached_count: cu√°ntos entraron por cach√©
            - total_time: tiempo total de la llamada
            - used_batch: n√∫mero de textos procesados en batch (no-cach√©)
        """
        logger.info(f"üîç Iniciando embed_texts para {len(texts)} textos")
        start_time = time.time()

        embeddings: List[List[float]] = []
        uncached: List[str] = []
        cache_map: dict[str, int] = {}

        try:
            # 1) Leer cach√©
            for idx, txt in enumerate(texts):
                cached = self.cache.get(txt)
                if cached is not None:
                    embeddings.append(cached.tolist())
                else:
                    embeddings.append(None)          # placeholder
                    cache_map[txt] = idx
                    uncached.append(txt)

            hits = len(texts) - len(uncached)
            logger.info(f"‚úÖ Cach√©: {hits} hits, {len(uncached)} misses")

            used_batch = 0
            # 2) Procesar no-cache en batches si existen
            if uncached:
                free_mem = psutil.virtual_memory().available
                # ejemplo de batch size din√°mico
                batch_size = min(len(uncached), max(1, int(free_mem / (768 * 4 * 1500))))
                batches = [
                    uncached[i : i + batch_size]
                    for i in range(0, len(uncached), batch_size)
                ]
                logger.info(f"üöÄ Procesando {len(uncached)} textos en {len(batches)} batches (batch_size={batch_size})")

                loop = __import__('asyncio').get_running_loop()
                tasks = [
                    loop.run_in_executor(
                        self.executor,
                        lambda bn=batch: self.model.encode(
                            bn,
                            batch_size=len(bn),
                            convert_to_numpy=True,
                            normalize_embeddings=True,
                            show_progress_bar=False
                        )
                    )
                    for batch in batches
                ]
                results = await __import__('asyncio').gather(*tasks)
                all_emb = np.vstack(results)

                # 3) Guardar en cach√© y completar embeddings
                for i, txt in enumerate(uncached):
                    pos = cache_map[txt]
                    emb = all_emb[i]
                    embeddings[pos] = emb.tolist()
                    self.cache.set(txt, emb)

                used_batch = len(uncached)
                logger.info(f"‚úÖ Batch completado, almacenados {used_batch} embeddings en cach√©")

            # C√°lculo de tiempos
            total_time = time.time() - start_time
            logger.info(f"‚è±Ô∏è Tiempo total embed_texts: {total_time:.2f}s")
            return embeddings, hits, total_time, used_batch

        except Exception as e:
            logger.error(f"‚ùå Error en embed_texts: {e}", exc_info=True)
            # Propagar la excepci√≥n para que el controller convierta en HTTPException si aplica
            raise
